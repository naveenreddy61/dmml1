# -*- coding: utf-8 -*-

import typing
from collections import defaultdict
from itertools import combinations

"""
Created on Thu Feb 13 14:30:58 2020

@author: naveenpc
"""

'''
dmml assignment 1
repository : https://archive.ics.uci.edu/ml/machine-learning-databases/bag-of-words/


For each text collection, D is the number of documents, W is the
number of words in the vocabulary, and N is the total number of words
in the collection (below, NNZ is the number of nonzero counts in the
bag-of-words). After tokenization and removal of stopwords, the
vocabulary of unique words was truncated by only keeping words that
occurred more than ten times. Individual document names (i.e. a
identifier for each docID) are not provided for copyright reasons.

These data sets have no class labels, and for copyright reasons no
filenames or other document-level metadata.  These data sets are ideal
for clustering and topic modeling experiments.

For each text collection we provide docword.*.txt (the bag of words
file in sparse format) and vocab.*.txt (the vocab file).

Enron Emails:
orig source: www.cs.cmu.edu/~enron
D=39861
W=28102
N=6,400,000 (approx)
'''


# global variables for minimum support and maximum size of frequent itemsets

MIN_SUP = 0.09
K = 6



def readtransactions(filename: str):
    '''
    input: filename like docword.kos.txt which contains docllist and wordlist
    output: D - number of documents in text collection
            W - number of words in the vocabulary
            N - total number of doc - word - count tuples starting from the next line
            transactions - dictionary of {doc1:{word1,word2}, doc2:{word2}}
    '''
    with open(filename) as f:
        D = int(f.readline())
        W = int(f.readline())
        N = int(f.readline())
        transactions = defaultdict(set)
        for line in f:
            transaction = list(map(int, line.split(' ')))
            transactions[transaction[0]].add(transaction[1])
    return (D, W, N, transactions)

def initialPass(transactions):
    #we are giving int() as a callable function to defaultdict whose default
    # value willl be 0.
    '''
    INput: given transactions like {doc1:{word1,word2}, doc2:{word2}}.
    generate and return candidates \
    which are 1 item sets which is basically all the unique words available.
    Output: {word1:{doc1},word2:{doc1,doc2}}
    '''
    candidates = defaultdict(set)
    for docId,wordIdList in transactions.items():
        for wordId in wordIdList:
            candidates[wordId].add(docId)
    return(candidates)


def candidategen2(F1, k):
    '''
    For generating candidates for 2 itemsets, they will be 2 combinations of
    frequent items in F1.
    C2 = 2 combinations of F1
    '''
    return list(combinations(F1, 2))


def candidategen(F,k):
    """
    Input: Frequent itemsets of size k-1
    Output: candidates of size k
    """
    l = len(F)
    candidates = set()
    F = sorted(list(F))
    for i in range(l):
        for  j in range(i+1,l):
            f1 = list(F[i])
            f2 = list(F[j])
            if f1[:-1] == f2[:-1]:
                c = f1 + [f2[-1]]
                count = 0
                for s in combinations(c, k-1):
                    if tuple(sorted(list(s))) not in F:
                        break
                    else:
                        count += 1
                if count == k:
                    candidates.add(tuple(c))
    return candidates

def apriori(transactions_info,support,K):

    if K <= 0:
        return []

    D,W,N,transactions = transactions_info
    #print('transactions are \n',transactions,'\n\n')
    candidates = initialPass(transactions)
    #print('candidates are',candidates,'\n\n')

    frequentItems = set()
    # adding frequent items of 1 size
    for candidate,docIds in candidates.items():
        if len(docIds)/D > support:
            frequentItems.add(candidate)
    #print("frequent items are:", frequentItems,"\n\n")
    # final itemsets
    itemsets = []
    itemsets.append(frequentItems)

    if K <= 1:
        print(len(itemsets[0]))
        return(itemsets)

    k = 2

    while k <= K and frequentItems != None :
        #give candidates as {1,2},{2,3}

        # kitemsets. is a set which contains itemsets of size k
        # frequentItems. type: initially it is {1,2}, then we give {(1,2),(2,3)}
        # candidates. candidates that are generated
        # count. dict which keeps a count of the k size items generated by candidategen
        # itemsets. final answer to which we add kitemsets
        kitemsets = set()
        if k == 2:
            candidates = candidategen2(frequentItems,k)
        else:
            candidates = candidategen(frequentItems,k)
        #print('candiates of ',k,' \n', list(candidates))

        count = defaultdict(int)

        for docId, wordIdSet in transactions.items():
            for candidate in candidates:
                if set(candidate) <= wordIdSet:
                    count[candidate] += 1
        #print('the count is ', count)
        for candidate, c in count.items():
            if c/D > support:
                kitemsets.add((candidate))
        #print('frequent items  of',k,' \n', kitemsets)
        itemsets.append(kitemsets)
        k += 1
        print(k)
        frequentItems = kitemsets
    #print(itemsets)
    return itemsets
'''
# TODO
# automate reading and storing docword files and vocab files
#D,W,N,transactions = readtransactions('docword.test.txt')
'''
def call_apriori(filename: str):
    #print(readtransactions(filename))
    apriori(readtransactions(filename),MIN_SUP,K)

#call_apriori('docword.test.txt')
call_apriori('docword.kos.txt')